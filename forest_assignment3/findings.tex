\documentclass[twocolumn]{article} 
\usepackage{fancyhdr} 
\usepackage{graphicx} 
\usepackage{epstopdf}                                                           

\textheight 9 in
\voffset -.75 in 

\title{CSCI 4320 - Parallel Programming\\Assignment 3}
\author{Forest Trimble\\trimbf@rpi.edu}
\date{\today}

\begin{document}

\maketitle

\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{Forest Trimble}
\fancyhead[C]{CSCI 4320-01}
\fancyhead[R]{\today}

This is intended to summarize the findings from using a memory-constrained matrix
multiply on Blue Gene/Q.  The data here were not generated by make, as the matrix 
multiply takes about 5 minutes to complete with 8 processors. However, if you'd 
like to regenerate the data, you can call \texttt{make run}, which will regenerate
the data for kratos on both 8 and 16 processors. Regenerating the 
data from Blue Gene/Q is obviously non-trivial, as you must run an \texttt{sbatch}
for the tasks, which can take time to get resource allocation. However, you can see
that in order to get my data on Blue Gene, I simply ran 
\texttt{bluegenescripts/bluegenerun.sh} (or \texttt{make blue}). Once the data was 
there, I tarballed it up and copied the tarballs into the \texttt{data/bluegene 
subdirectory}, where the \texttt{kratosscripts/getdata.sh} script converts the raw
data into the files that our plotting script takes. All that being said, you can 
also simply regenerate the plots from the old data using \texttt{make kratos}, and
you can perform a smaller version of the matrix multiply using \texttt{make quick}.
Note that the results are outputted into \texttt{quick.out}. Finally, note that
all plots in this graph are log scale on both axes. \\

Now we get to the fun part; consider the execution time plots below. The data 
points were calculated in the following ways:
\begin{itemize}
\item Execution time started after running \texttt{MPI\_Init}, 
  \texttt{MPI\_Comm\_rank}, and \texttt{MPI\_Comm\_size}, and ended after the 
  matrix multiply and sends/recvs loop completed.
\item Average Time was calculated by performing a reduce to task 0 that summed
  the execution time of each task, and then divided by numtasks. 
\item Minimum Time was calculated by performing a reduce to task 0 that minned
  the execution time of tasks
\item Maximum Time was calculated by performing a reduce to task 0 that maxed
  the execution time of tasks
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/execAvg.eps}
  \caption{Execution Time Data on Kratos}
\end{figure}

As you can see, the response to scale in kratos is an entirely beneficial one: 
kratos does not have the sheer quantity of processors necessary to make the 
communication overhead a bottleneck for performance. However, our response to 
doubling the number of tasks was nowhere near a halving of time. \\

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/bluegene/experiment1/execAvg.eps}
  \caption{Execution Time Data on BlueGene}
\end{figure}

However, when we look at the impact of scale in the Blue Gene/Q on experiment 1, 
we notice that there are most definitely diminishing returns. Once we increase 
past 512 tasks, we get a negligible improvement in execution time. Since I have 
not separated out each graph by the number of tasks per processors that are used,
it is a bit difficult to tell if there is also an impact due to overcommitting.
Nonetheless, the evidence that we are running into a communication bottleneck as
we increase the number of tasks is overwhelming. \\

In experiment 2, the same analysis holds for the most part, but we can see a single
number of tasks per processor through all node configurations. Perhaps the most 
interesting thing to see here is that experiment 2 completes \emph{faster} than
experiment 1. This is quite interesting as all our torus mappings are no longer
adjacent. According to the Blue Gene/Q manual, however, this is actually something
to be expected. \\

Comparing the difference in Blue Gene vs Kratos is a bit difficult, as Kratos is 
running on a far smaller quantity of processors, but you can see that Kratos' 
execution time is approximately 4x that of the Blue Gene execution time on the
smallest number of processors, which is about the ratio of the change in number 
of tasks. However, the benefit of adding new tasks on Kratos appears to be
significantly lower than on Blue Gene, most likely due to the seriously impressive
network on Blue Gene. Indeed, the drop in time due to increasing from 8 to 16
tasks was nowhere near quartering it. Thus it is unlikely that Any extension of 
Kratos' capabilities would lead to speeds close to those of Blue Gene. This is
quite unsurprising, as this is exactly what Blue Gene was made for. \\

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/commAvg.eps}
  \caption{Kratos Communication Bandwidth Data}
\end{figure}

Here are some plots about bandwidth as well. Here's how they were calculated:
\begin{itemize}
\item This is dedicated communication time, which is calculated by starting a
  clock before sending out \texttt{MPI\_Isend} and \texttt{MPI\_Irecv} signals,
  and ending it after those calls finish. The timer also starts before waiting
  for any calls and ends after waiting is done. 
\item Bandwidth was calculated as follows:
  \[%\mbox{ \footnotesize{Bandwidth}} = 
  \frac{(\mbox{\footnotesize{Matrix Size}})^2((\mbox{\footnotesize{Number of Tasks}})-1)(\mbox{\footnotesize{Size of a Double}})}{(\mbox{\footnotesize{Number of Tasks}})(\mbox{\footnotesize{Communication Time}})} \]
  Which followed from the fact that we sent out a chunk of data that was 
  \[ \frac{(\mbox{\footnotesize{Matrix Size}})^2(\mbox{\footnotesize{Size of a Double}})}{\mbox{\footnotesize{Number of Tasks}}} \]
  in size $(\mbox{Number of Tasks})-1$ times, which is the total amount of data
  sent out. Dividing data by time yields bandwidth.
\item The average, minimum, and maximum were found by performing a reduce, as in
  the execution time data. 
\end{itemize}

As you can see from the graphs, effective bandwidth on Kratos continued to increase 
as we scaled processors up. It is likely that if we had the continued ability to 
add more processors on kratos, we would see a cap when communication overhead 
started to become a bottleneck.\\

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/bluegene/experiment1/commAvg.eps}
  \caption{Blue Gene Bandwidth Data}
\end{figure}

Note that, contrary to the spec given in the assignment, I do not have two separate
plots for send and recv bandwidth. This was a conscious decision due to the 
non-blocking nature of I\_Send and I\_Recv; separate plots would not make sense, as
they complete concurrently. Any data about one of them necessarily depends on the 
other. \\

When you consider instead the effective communication bandwidth on Blue Gene 
during experiment 1, you see a much more informative plot. Indeed, at the smallest
quantity of processors, it is quite apparent that the matrix multiplication is 
taking enough time that the communication is pretty much entirely complete before
we are done, as we increase the number of tasks, however, we notice that effective
bandwidth drops significantly. Clearly, we are starting to experience the impact 
of the communication overhead far outweighing the gains of reducing the number of
FLOPS per task. \\

The results from experiment 2 are quite a bit different. It appears that the default
ABCDET mapping allows us to maintain a low communication overhead, even when the 
number of floating point operations per processor is very low. While we do still 
experience an order of magnitude decrease in effective bandwidth, it is a trivial
reduction compared to in experiment 1, where the decrease was several orders of 
magnitude. This does a great deal in helping to explain why execution time in 
experiment 2 did not level off in the same way that it did in experiment 1. \\

What is interesting to note is how for all number of tasks in experiment 2, the
effective bandwidth is orders of magnitude larger than the effective bandwidth on
Kratos. Again, we can point to the far more well developed network on Blue Gene/Q,
but when we believe that part of the cause is that most of the communication 
happens during the matrix multiply, it seems like a somewhat strange result. \\

Here are some plots about how much time was spent actually multiplying matrices
together. Here's how the data was calculated:
\begin{itemize}
\item The time was calculated by starting a timer right before we began our 
  triply nested for loop and ending it right after the for loops. These times 
  were summed over all the blocks that we went through. 
\item The average, minimum, and maximum were found by performing a reduce as 
  in the execution time data.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/mmAvg.eps}
  \caption{Kratos Matrix Multiply Time Data}
\end{figure}

Unfortunately, two data points make it fairly hard to extrapolate any real trends 
from the data as we increase in scale, but you can see that kratos is not 
extremely receptive to scale, as we noticed in the other two plots. \\

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/bluegene/experiment1/mmAvg.eps}
  \caption{Blue Gene Matrix Multiply Time Data}
\end{figure}

Observing the data from both experiments on the Blue Gene is quite intuitive. We 
see that the trend is that every time we double the number of tasks, the time is
approximately halved. This is entirely unsurprising, as this involves no 
communication overhead; it is measuring a series of floating point operations. Since
we are halving the number of floating point operations per task every time we 
double the number of tasks, This is exactly what we expect. Any fluctuation is due
only to OS jitter. An interesting thing to note is that we can see how much less 
prevalent OS jitter is on the Blue Gene/Q than on Kratos, as the results are almost
\emph{exactly} halved for every doubling of tasks on Blue Gene, whereas the effect
was not even a reduction by $75\%$ on Kratos. 
 

\end{document}
