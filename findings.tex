\documentclass[twocolumn]{article} 
\usepackage{fancyhdr} 
\usepackage{graphicx} 
\usepackage{epstopdf}                                                           

\textheight 9 in
\voffset -.75 in 

\title{CSCI 4320 - Parallel Programming\\Assignment 4}
\author{Forest Trimble, David Vorick, Scott Todd\\\{trimbf,voricd,todds\}@rpi.edu}
\date{\today}

\begin{document}

\maketitle

\pagestyle{fancy}
\fancyhead{}
\fancyhead[C]{Forest Trimble, David Vorick, Scott Todd}
\fancyhead[L]{CSCI 4320-01}
\fancyhead[R]{\today}

This document is intended to summarize the findings from using a 
memory-constrained matrix multiplication on the Blue Gene/Q. The data here were 
generated on Blue Gene/Q, using \texttt{make blue}. Since we only required one 
node and the matrix multiply was fairly small (only 1024 by 1024), these runs
can run through sbatch and completion very quickly. Using \texttt{make} here will
regenerate the data, and using \texttt{make kratos} or \texttt{make quick} 
will perform a smaller version of the matrix multiply (512 by 512) and regenerate
the plots. Note that the x axis for the plots is on a $\log_2$ scale.\\

Now we begin our analysis. Consider the plot of execution times in Figure 
\ref{fig:exec}. One can see that there is an obvious performance decrease when we
use too many threads. It appears that the Q has a limited ability to share memory 
between cores on a node, so it is likely that the overhead of having to do that 
far outweighs the benefits of performing the matrix multiply. I am a bit surprised,
however, that 8 is the optimal configuration. The Q has 16 cores per node, each with
4 threads, so it would seem that if the computational overhead of shared memory was
a problem, it would become a problem at 8 nodes, not 16. \\

If we recall our data from last time, almost the entirety of our 
execution time was spent in the matrix-multiply, and communication was a 
non-factor based on how quickly it proceeded. Using threading should reduce the 
time spent on matrix multiplication; it is only a question of how successful the
shared memory among the threads in our node are. Clearly, the overhead for sharing
memory on the Q around an entire node takes up more time than the actual matrix 
multiplication! Additionally, you can see that using only a few threads per node, 
where the Q has the infrastructure so that shared memory is quite efficient, is an
impressive boon on performance! It looks like by swapping to a configuration with
about 8 threads per node we earn the highest performance tradeoff, reducing 
execution time by about half. \\

I am a bit surprised, however, that we did not see 
more significant performance gains for using 8 threads per node, as it would seem 
that such a configuration would be more than capable of reducing matrix 
multiplication time without tasking the network further. Interestingly, it may
have been possible that, while communication overhead was a non-factor in the 
previous assignment, the division that we were using was exactly sufficient to
task the network to capacity, so reducing matrix multiplication time by a factor
of 8 would not receive the same gains to overall time. \\

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/exec.eps}
  \caption{Execution times for various threading configurations} \label{fig:exec}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[height=2in]{plots/execK.eps}
  \caption{Execution times for various threading configurations on Kratos} 
\end{figure}

It is interesting to note the results that we see on Kratos; it appears that Kratos
has a processor setup where having more than 4 threads per
core is just not helpful. Note that we have seen some variation in these results,
and since they are compiled on the spot, you may see something slightly different,
but this is by far the most prevalent pattern we have seen.\\

After analyzing the performance of various thread configurations, our next task
was to test the success of our chosen arrangement by attempting to multiply the
largest possible matrix with our given configuration. \\

Refer to Figure \ref{fig:time} for the statistics on matrix size vs. time. You can
see that the blue gene was able to compute the 32768 by 32768 matrix in a little 
over 4 minutes, but when we increased the size up to the full 64k matrix, we took
almost half an hour! Clearly, while we do benefit from serious gains by using the
optimal number of threads per node, it does not solve everything. Indeed, this is
a constraint that occurs with parallel programming in general: while we can speedup
performance by enormous quantities, it is only ever by a constant factor. Thus, 
intractable problems are still intractable problems, we simply have the advantage of
being able to solve slightly larger versions of them. \\

Unsurprisingly, we see a large increase in execution time as we increase the size
of the matrix on Kratos. In the interests of speed of \texttt{make quick}, we have
opted to not show the full ability of Kratos in ten minutes, but you can see that 
the 4096 by 4096 matrix computes in under 30 seconds. Since Matrix Multiply is 
cubic by nature, one would assume that doubling the size to 8192 by 8192 would 
take approximately 8 times as much, or about 4 minutes. While we would not be able
to compute a matrix double in size again within ten minutes, this does appear to 
be faster than our original 8192 by 8192 matrix from assignment 3.

\begin{figure}
  \includegraphics[height=2in]{plots/sizes.eps}
  \caption{Execution times for various matrix sizes} \label{fig:time}
\end{figure}

\begin{figure}
  \includegraphics[height=2in]{plots/sizeK.eps}
  \caption{Execution times for various matrix sizes on Kratos}
\end{figure}


\end{document}
